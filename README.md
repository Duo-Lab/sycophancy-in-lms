# Evaluating Sycophancy in Language Models

This repository explores **sycophancy in language models (LMs)** , the tendency of models to agree with user opinions blindly.  
We investigate whether **fine-tuning on general benchmarks** (e.g., MMLU, GSM8K, HellaSwag, TruthfulQA) affects a modelâ€™s **truthfulness and independence**.

---

## Project Overview

The core goal of this project is to **measure and compare sycophantic behavior** in a language model **before and after fine-tuning**.

We aim to answer:

> Does fine-tuning an SLM Model to maximize performance on general capability benchmarks lead to a statistically significant increase in sycophantic behavior, compared to its baseline version, as measured by SycophancyEval?
